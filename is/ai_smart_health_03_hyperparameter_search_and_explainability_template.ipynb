{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß†üí° Intelligent Systems  for Smart Health üë®‚Äç‚öïüë©‚Äç‚öïÔ∏èüî¨üå°Ô∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding hyperparameters and understanding models\n",
    "\n",
    "In the last weeks we trained linear models, decision tree and a random forest. In all those cases we were manually adjusting hyperparameters. The more complex our models get... the more inefficient this procedure becomes. So we will look at **grid search** and **random search** as two common techniques for (still fairly simple) hyperparameter searches.\n",
    "\n",
    "In the second part of this session we will then ask the question of how to make sense of the predictions we get from such an optimized machine learning model. Why does one patient get a good prognosis, and another one a bad one?\n",
    "In this context, we will look at the **feature importance** (for a random forest model) and and **SHAP**, a newer technique to interpret model predictions.\n",
    "\n",
    "We will (again) work with actual medical data in this notebook, namely the NHANES I epidemiology dataset (for a detailed description of this dataset you can check the [CDC Website](https://wwwn.cdc.gov/nchs/nhanes/nhefs/default.aspx/)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6k2pItifWeK"
   },
   "source": [
    "<a name='import'></a>\n",
    "## Import Packages\n",
    "\n",
    "We'll first import all the common packages that we need for this assignment. \n",
    "\n",
    "- `shap` is a library that explains predictions made by machine learning models.\n",
    "- `sklearn` is one of the most popular machine learning libraries.\n",
    "- `itertools` allows us to conveniently manipulate iterable objects such as lists.\n",
    "- `pydotplus` is used together with `IPython.display.Image` to visualize graph structures such as decision trees.\n",
    "- `numpy` is a fundamental package for scientific computing in Python.\n",
    "- `pandas` is what we'll use to manipulate our data.\n",
    "- `seaborn` is a plotting library which has some convenient functions for visualizing missing data.\n",
    "- `matplotlib` is a plotting library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydotplus\n",
    "#!pip install lifelines\n",
    "#!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5s0iQ82okBv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import sklearn\n",
    "import itertools\n",
    "import pydotplus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image \n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1. The Dataset\n",
    "### Load and explore the data!\n",
    "\n",
    "In virtually all cases, we would first want to get an intuition on the data itself. Things like: What is in the data? How much data is there? Are there things missing? What might cause problems? Do we understand the type of data/features?\n",
    "\n",
    "With **pandas**, we usually can explore some key properties very rapidly, for instance with commands like\n",
    "\n",
    "- `data.head()`\n",
    "- `data.describe()`\n",
    "- `data.info()`\n",
    "\n",
    "### Some weird conventions:\n",
    "For some reason it became standard in the machine learning world to name the actual data `X` and the labels `y`. Even though I consider this a rather poor choice both from a math and from a code best practice point of view, we will stick to this in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"../data/\"\n",
    "\n",
    "X = pd.read_csv(os.path.join(path_data, \"NHANESI_subset_X.csv\"))\n",
    "y = pd.read_csv(os.path.join(path_data, \"NHANESI_subset_y.csv\"))\n",
    "\n",
    "X = X.drop('Unnamed: 0', axis=1)\n",
    "y = y.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X.copy()\n",
    "data[\"time\"] = y\n",
    "data = data.dropna(axis=\"rows\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 10  # time period we consider --> we focus on >10 year risk\n",
    "\n",
    "def died_in_period(time, period):\n",
    "    # Option 1: Person died within a period\n",
    "    if time > 0 and time < period:\n",
    "        return 1\n",
    "\n",
    "    # Option 2: Person left study before period ended --> no conclusion possible\n",
    "    if time <= 0 and time > -period:\n",
    "        return np.nan\n",
    "\n",
    "    # Option 3 + 4: Person left study after >= period or survived >= period\n",
    "    return 0\n",
    "    \n",
    "death_in_period = np.array([died_in_period(time, period) for time in data.time])\n",
    "death_in_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"death_in_period\"] = death_in_period\n",
    "\n",
    "# Remove missing entries\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.death_in_period.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data/label split --> X, y\n",
    "X = data.drop([\"death_in_period\", \"time\"], axis='columns')\n",
    "y = data[\"death_in_period\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.15,\n",
    "    random_state=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_dev, y_dev,\n",
    "    test_size=0.18,\n",
    "    random_state=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this context we will also use the c-index to evaluate our models\n",
    "import lifelines\n",
    "\n",
    "def cindex(y_true, scores):\n",
    "    return lifelines.utils.concordance_index(y_true, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2. Random Forests\n",
    "\n",
    "As we saw last time, a single decision tree is prone to overfitting. To solve this problem, you can try **random forests**, which combine predictions from many different trees to create a robust classifier. \n",
    "\n",
    "As before, we will use scikit-learn to build a random forest for the data. We will use the default hyperparameters.\n",
    "\n",
    "Using Scikit-Learn we can train such a model using the `RandomForestClassifier()` class and, again, the `fit()` method to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please train a random forest model using the RandomForestClassifier() class\n",
    "# start with 10 trees --> n_estimators=10\n",
    "forest = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic first model evaluation\n",
    "Here, for a start, we will simply use the C-Index. Feel free to add other measures (e.g., accuracy etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = forest.predict_proba(X_train)[:, 1]\n",
    "print(f\"Train C-Index: {cindex(y_train.values, y_train_preds)}\")\n",
    "\n",
    "y_val_preds = forest.predict_proba(X_val)[:, 1]\n",
    "print(f\"Val C-Index: {cindex(y_val.values, y_val_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a random forest with the default hyperparameters results in a model that has better predictive performance than individual decision trees as in the previous section, but this model is overfitting.\n",
    "\n",
    "We therefore need to tune (or optimize) the hyperparameters, to find a model that both has good predictive performance and minimizes overfitting.\n",
    "\n",
    "The hyperparameters we choose to adjust will be:\n",
    "\n",
    "- `n_estimators`: the number of trees used in the forest.\n",
    "- `max_depth`: the maximum depth of each tree.\n",
    "- `min_samples_leaf`: the minimum number (if `int`) or proportion (if `float`) of samples in a leaf.\n",
    "\n",
    "\n",
    "<a name='ex1'></a>\n",
    "### Exercise 1: try to find a set of better hyperparameters!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train some models using different hyperparameters\n",
    "forest = # add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = forest.predict_proba(X_train)[:, 1]\n",
    "print(f\"Train C-Index: {cindex(y_train.values, y_train_preds)}\")\n",
    "\n",
    "y_val_preds = forest.predict_proba(X_val)[:, 1]\n",
    "print(f\"Val C-Index: {cindex(y_val.values, y_val_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3.Systematic search for hyperparameters: grid search\n",
    "\n",
    "The approach we implement to tune the hyperparameters is known as a grid search:\n",
    "\n",
    "- We define a set of possible values for each of the target hyperparameters.\n",
    "\n",
    "- A model is trained and evaluated for every possible combination of hyperparameters.\n",
    "\n",
    "- The best performing set of hyperparameters is returned.\n",
    "\n",
    "The cell below implements a hyperparameter grid search, using the C-Index to evaluate each tested model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;max_depth&#x27;: [4, 8, 12, 20],\n",
       "                         &#x27;min_samples_leaf&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [2, 10, 100]},\n",
       "             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;max_depth&#x27;: [4, 8, 12, 20],\n",
       "                         &#x27;min_samples_leaf&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [2, 10, 100]},\n",
       "             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [4, 8, 12, 20],\n",
       "                         'min_samples_leaf': [2, 5, 10],\n",
       "                         'n_estimators': [2, 10, 100]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\"max_depth\": ...\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model out of the grid search with .best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = # add code to use the best model\n",
    "\n",
    "y_train_preds = forest.predict_proba(X_train)[:, 1]\n",
    "print(f\"Train C-Index: {cindex(y_train.values, y_train_preds)}\")\n",
    "\n",
    "y_val_preds = forest.predict_proba(X_val)[:, 1]\n",
    "print(f\"Val C-Index: {cindex(y_val.values, y_val_preds)}\")\n",
    "\n",
    "y_test_preds = forest.predict_proba(X_test)[:, 1]\n",
    "print(f\"Test C-Index: {cindex(y_test.values, y_test_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect grid search results\n",
    "cv_results = grid_search.cv_results_\n",
    "mean_test_scores = cv_results['mean_test_score']\n",
    "params = cv_results['params']\n",
    "\n",
    "# Prepare data for plotting\n",
    "scores_array = np.array(mean_test_scores).reshape(len(parameters['max_depth']),\n",
    "                                                  len(parameters['min_samples_leaf']),\n",
    "                                                  len(parameters['n_estimators']))\n",
    "\n",
    "# Create a line plot for each n_estimators\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for k, n_estimators in enumerate(parameters['n_estimators']):\n",
    "    scores_for_n_estimators = scores_array[:, :, k].T\n",
    "    for i, min_samples_leaf in enumerate(parameters['min_samples_leaf']):\n",
    "        ax.plot(parameters['max_depth'], scores_for_n_estimators[i],\n",
    "                marker='o', linestyle='--', label=f'n_estimators: {n_estimators}, min_samples_leaf: {min_samples_leaf}')\n",
    "\n",
    "ax.set_title('Grid Search Results')\n",
    "ax.set_xlabel('Max Depth')\n",
    "ax.set_ylabel('Mean Test Score')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "A common alternative to the simple grid search is a more randomized search. The version that is implemented in Scikit-learn is a bit of a hybrid. It can be seen as a randomized search within a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "parameters = {\"max_depth\": [],\n",
    "              \"min_samples_leaf\": [],\n",
    "              \"n_estimators\": []\n",
    "             }\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "random_search = # add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel = \"n_estimators\"\n",
    "ylabel = \"max_depth\"\n",
    "zlabel = \"min_samples_leaf\"\n",
    "x_pos = []\n",
    "y_pos = []\n",
    "z_pos = []\n",
    "for param in random_search.cv_results_[\"params\"]:\n",
    "    x_pos.append(param[xlabel])\n",
    "    y_pos.append(param[ylabel])\n",
    "    z_pos.append(param[zlabel])\n",
    "\n",
    "plt.scatter(x_pos, y_pos, s=50*np.array(z_pos),\n",
    "            c=random_search.cv_results_[\"mean_test_score\"])\n",
    "plt.colorbar(label=\"mean test score\")\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel)\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the \"best\" model\n",
    "model = # add your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optional (if time) Training a lightGBM model\n",
    "\n",
    "XGBoost models became somewhat famous on platforms such as Kaggle, because they were very often found in the top ranks of the leadboards. So let's give this a try as well!\n",
    "\n",
    "https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5. Explainability\n",
    "Using a random forest has improved results, but we've lost some of the natural interpretability of trees. In this section we'll try to explain the predictions using slightly more sophisticated techniques. \n",
    "\n",
    "But first, we will simply look at the **feature importance**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Just to all be on the same page: let's train one last model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100,\n",
    "                               max_depth=7,\n",
    "                               min_samples_leaf=5,\n",
    "                               random_state=10)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .feature_importances_ to get the respective values of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(# add feature importances here,\n",
    "                                 columns=[\"feature_importance\"],\n",
    "                                 index=X_train.columns)\n",
    "feature_importance.sort_values(\"feature_importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFM27SfS1QSD"
   },
   "source": [
    "<a name='shap'></a>\n",
    "## Better than only feature importance: SHAP\n",
    "\n",
    "Looking at the feature importance was interesting, and tells us a bit more about the model.\n",
    "But since recently, we can do much better!\n",
    "\n",
    "Now we will apply **SHAP (SHapley Additive exPlanations)**, a cutting edge method that explains predictions made by black-box machine learning models (i.e. models which are too complex to be understandable by humans as is).\n",
    "\n",
    "> Given a prediction made by a machine learning model, SHAP values explain the prediction by quantifying the additive importance of each feature to the prediction. SHAP values have their roots in cooperative game theory, where Shapley values are used to quantify the contribution of each player to the game.\n",
    "> \n",
    "> Although it is computationally expensive to compute SHAP values for general black-box models, in the case of trees and forests there exists a fast polynomial-time algorithm. For more details, see the [TreeShap paper](https://arxiv.org/pdf/1802.03888.pdf).\n",
    "\n",
    "We'll use the [shap library](https://github.com/slundberg/shap) to do this for our random forest model. Run the next cell to output the most at risk individuals in the test set according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_death = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "emlK-wlyJEel",
    "outputId": "5de57232-6d03-4c0f-d438-ba4ad393d6af"
   },
   "outputs": [],
   "source": [
    "X_test_risk = X_test.copy(deep=True)\n",
    "X_test_risk[\"predicted_risk\"] = proba_death\n",
    "X_test_risk = X_test_risk.sort_values(\"predicted_risk\", ascending=False)\n",
    "X_test_risk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aefDv0PDKrfv"
   },
   "source": [
    "We can use SHAP values to try and understand the model output on specific individuals using force plots. Run the cell below to see a force plot on the riskiest individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "patientID = X_test_risk.index[i]\n",
    "print(patientID)\n",
    "print(X_test.loc[patientID, :], \"\\n\")\n",
    "print(f\"Our model predicts: {model.predict(X_test.loc[[patientID]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why this prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SHAP's general \"Explainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "colab_type": "code",
    "id": "elJX1FqWKzYm",
    "outputId": "7c97d958-9383-4d7a-bb2b-e61af5ecf47f"
   },
   "outputs": [],
   "source": [
    "explainer = # add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = # add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_class = 1\n",
    "\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[prediction_class], shap_values[:, prediction_class],\n",
    "    feature_names=X_test.columns,\n",
    "    matplotlib=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SHAP's \"TreeExplainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = # add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    explainer.expected_value[prediction_class], shap_value,\n",
    "    feature_names=X_test.columns,\n",
    "    matplotlib=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(...)\n",
    "shap_values = explainer(..., check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(...)\n",
    "\n",
    "shap.summary_plot(...)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-gc0vttU7Dkf",
    "CFM27SfS1QSD"
   ],
   "include_colab_link": true,
   "name": "C2_W2_Assignment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
