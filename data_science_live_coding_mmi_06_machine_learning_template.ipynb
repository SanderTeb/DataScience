{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42d0ce7",
   "metadata": {},
   "source": [
    "# Introduction machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5fcf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e43eb",
   "metadata": {},
   "source": [
    "## Importing data\n",
    "\n",
    "The data can be imported using Pandas with the command `pd.read_csv()`.\n",
    "In many cases, this does not work directly. This is usually due to one of the following issues:\n",
    "- `FileNotFoundError` --> Either the file name is spelled incorrectly or the path is incorrect.\n",
    "- `UnicodeDecodeError` --> Either the file name (+path) contains invalid characters (in Windows, for example, \"//\" must often be used instead of \"/\"), or the file itself is not saved in the expected \"encoding.\" For the latter, there are two options: (1) Convert the file with an editor. Or (2) set the parameter `encoding=...` parameter.  \n",
    "There are many possible encodings ([see link](https://docs.python.org/3/library/codecs.html#standard-encodings)), but the most common are \"utf-8\" (the standard), \"ANSI\" (on Mac: \"iso-8859-1\" or ‘ISO8859’) or \"ASCII\".\n",
    "- `ParserError` --> Usually means that the \"delimiter\" (i.e., the separator) is specified incorrectly. It is best to open the file briefly with an editor and check, then set it accordingly with `delimiter=\"...\"` (or `sep=\"...\"`). Typical separators are `\",\"`, `\";\"`, `\"\\t\"` (tab).\n",
    "- If the file does not start with the desired column names, this can be corrected by specifying the rows to be skipped --> `skiprows=1` (1, 2, 3,... depending on the case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8ef306-9119-4145-ae67-2a5c8ed9e58a",
   "metadata": {},
   "source": [
    "## Titanic dataset!\n",
    "\n",
    "This data is taken from the [Kaggle Titanic challenge](https://www.kaggle.com/c/titanic/data).\n",
    "\n",
    "Here, we will attempt to predict whether passengers survived the Titanic disaster based on their passenger data.\n",
    "\n",
    "### Data Dictionary\n",
    "\n",
    "| Variable   | Definition                        | Key                                        |\n",
    "|------------|-----------------------------------|--------------------------------------------|\n",
    "| survival   | Survival                          | 0 = No, 1 = Yes                            |\n",
    "| pclass     | Ticket class                      | 1 = 1st, 2 = 2nd, 3 = 3rd                  |\n",
    "| sex        | Sex                               |                                            |\n",
    "| age        | Age in years                      |                                            |\n",
    "| sibsp      | # of siblings/spouses aboard the Titanic |                                      |\n",
    "| parch      | # of parents/children aboard the Titanic |                                      |\n",
    "| ticket     | Ticket number                     |                                            |\n",
    "| fare       | Passenger fare                    |                                            |\n",
    "| cabin      | Cabin number                      |                                            |\n",
    "| embarked   | Port of Embarkation               | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "### Variable Notes\n",
    "\n",
    "- **pclass:** A proxy for socio-economic status (SES)\n",
    "  - 1st = Upper\n",
    "  - 2nd = Middle\n",
    "  - 3rd = Lower\n",
    "\n",
    "- **age:** Age is fractional if less than 1. If the age is estimated, it is in the form of `xx.5`.\n",
    "\n",
    "- **sibsp:** The dataset defines family relations in this way:\n",
    "  - Sibling = brother, sister, stepbrother, stepsister\n",
    "  - Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "\n",
    "- **parch:** The dataset defines family relations in this way:\n",
    "  - Parent = mother, father\n",
    "  - Child = daughter, son, stepdaughter, stepson\n",
    "  - Some children traveled only with a nanny, therefore `parch=0` for them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6661150",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data\\\\titanic_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m path_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitanic_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\Sander\\Repos\\DataScience\\.conda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sander\\Repos\\DataScience\\.conda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Sander\\Repos\\DataScience\\.conda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sander\\Repos\\DataScience\\.conda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Sander\\Repos\\DataScience\\.conda\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data\\\\titanic_train.csv'"
     ]
    }
   ],
   "source": [
    "path_data = \"/Data\"\n",
    "filename = os.path.join(path_data, \"titanic_train.csv\")\n",
    "\n",
    "data = pd.read_csv(filename)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a164715",
   "metadata": {},
   "source": [
    "# (1) Initial data exploration\n",
    "This should now be almost automatic.\n",
    "\n",
    "- Are there any missing values? --> `.info()`\n",
    "- Initial overview & search for problematic entries --> `.describe()` (or `.describe(include=\"all\")`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90980b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271fd4df-57b2-457d-8758-876fdffcd147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f1c914e",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "We need to make some decisions here!\n",
    "\n",
    "- Columns in which we have very few entries --> remove\n",
    "- Remove columns that we deliberately do not want to use for our predictions --> `Name`, `Ticket`\n",
    "- Problem case: `Age` --> Here, as an exception, we want to estimate the missing values. This is called **data imputation** and should be avoided in most cases, as it adds generated values, which are essentially *fake data*. However, in this case, please fill in the missing values with `fillna()` using the average age of all other entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c786f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2b780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e33c0a3-030e-4716-be04-f09f09b10f57",
   "metadata": {},
   "source": [
    "### Convert categorical data\n",
    "We still have columns with categorical entries (as strings). These need to be converted to numerical values using `pd.get_dummies`.\n",
    "\n",
    "Tip: Avoid duplicating the same information. So there is no need for \"Sex_male\" AND \"Sex_female\" as one of the two pieces of information is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b9169a-8238-4896-9f43-a24981cc9dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4f97f15",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ffe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here only mildly informative... but feel free to try\n",
    "# sb.pairplot(data_cleaned, hue=\"Survived\", diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac681c3-85b8-43a2-84d4-8382b83d5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned[\"Survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcb8b1-dd36-428e-8c73-b3b95834abd6",
   "metadata": {},
   "source": [
    "## Correlation matrix\n",
    "\n",
    "Based solely on correlations: \n",
    "**Which features can we expect to play a role in predicting survival (`Survived`)?**\n",
    "\n",
    "**Which feature appears to be the most important?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3d8c0-daa5-4874-ab27-815bf29e0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "corr_matrix = data_cleaned.corr(numeric_only=True)\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sb.heatmap(corr_matrix,\n",
    "           mask=mask,\n",
    "           annot=True,\n",
    "           vmin=-1, vmax=1,\n",
    "           square=True,\n",
    "           cmap=\"RdBu\",\n",
    "           linewidths=.5, fmt=\".1f\", ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece8654",
   "metadata": {},
   "source": [
    "# Split into data and labels\n",
    "\n",
    "- Label: \"Survived\" --> 0 did not survive | 1 survived\n",
    "- Data: Everything except \"Survived\" --> `.drop()`\n",
    "\n",
    "### Tasks:\n",
    "- Create the data `X` and the labels `y` from `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57afce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "y = data_cleaned[\"Survived\"]\n",
    "\n",
    "# data\n",
    "X = data_cleaned.drop([\"Survived\"], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa530c2",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "The scikit-learn function `train_test_split` randomly divides a data set into training and test data. We can specify the proportion of test data using `test_size=...`, where values between 0 (no data) and 1 (all data) are used.\n",
    "See also the [Scikit-Learn documentation on train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "Since this is a random distribution, it is better to set a \"seed\" to make it reproducible, using `random_state=0` (or another number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117877b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = # TODO: add your code\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0a14c",
   "metadata": {},
   "source": [
    "# Training of a kNN model\n",
    "## Scaling data\n",
    "\n",
    "For some algorithms, it is very important that the data is all scaled similarly. This is also the case for k-nearest neighbors, for example. To do this, we again use the `StandardScaler` from Scikit-Learn.\n",
    "\n",
    "The \"cleanest\" approach here is to perform the scaling **based on the training data** so that no indirect information from the test data is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af40ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()...\n",
    "\n",
    "X_train = pd.DataFrame(# complete code here,\n",
    "                       columns=X.columns)\n",
    "X_test = pd.DataFrame(# complete code here,\n",
    "                      columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38221258",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea7c30",
   "metadata": {},
   "source": [
    "# Train model\n",
    "First, we will try out a k-nearest neighbor model, again using scikit-learn. See [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier).\n",
    "The most important parameter is `n_neighbors`, i.e., the number of neighbors (the `k` in k-NN).\n",
    "\n",
    "### Task:\n",
    "- Train a k-nearest neighbor model with the training data. This means creating a `KNeighborsClassifier` object (with the necessary parameters) and then training it with `.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = # complete code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aaa359",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "While we train a model with `.fit()`, we can make predictions with `.predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_survival = knn. # complete code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c7596",
   "metadata": {},
   "source": [
    "## Evaluate results\n",
    "\n",
    "A good way to check classification predictions is the confusion matrix.To do thisconfusion_matrix()and pass it the actual labels and the predicted labels as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289dd668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(# add code here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c853658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which prediction classes were learnt by the model\n",
    "knn.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27598cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "sb.heatmap(confusion_matrix(# add code here),\n",
    "           annot=True, cmap=\"Blues\", cbar=False, fmt=\".0f\",\n",
    "           xticklabels=[\"Died\", \"Survived\"],\n",
    "           yticklabels=[\"Died\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225d4e6",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decision%20tree#sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d117194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=#add,\n",
    "    random_state=0\n",
    ") \n",
    "\n",
    "# Important! Decision trees need no data scaling!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aee2b4-caa2-4bc4-a07f-f732ff99b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ae7b1-53cd-4130-b7b0-ca561ce5d9ea",
   "metadata": {},
   "source": [
    "### First train a decision tree WITHOUT setting any parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79acd7c-a215-4a33-8661-c6946dd201f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()  # do NOT add any parameters here --> we will use the default settings\n",
    "tree.fit(# add code here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d60d93-1111-4f38-861b-af5027e978e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's make some predictions...\n",
    "prediction_survival = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec5683-bad9-49df-bfd9-758578cb324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "sb.heatmap(confusion_matrix(y_train, prediction_survival),\n",
    "           annot=True, cmap=\"Blues\", cbar=False, fmt=\".0f\",\n",
    "           xticklabels=[\"Died\", \"Survived\"],\n",
    "           yticklabels=[\"Died\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26f560-8e28-41f7-98e1-1d98b58d70c0",
   "metadata": {},
   "source": [
    "### Evaluation:\n",
    "Looks like the model is pretty good. What else would we need to check to be sure?\n",
    "\n",
    "- Take a look at the same thing, but this time for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0dd02-ffb2-4d15-9ae8-d1c678e82257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c6465-e1e1-4219-a544-12304851a174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d237528",
   "metadata": {},
   "source": [
    "# Train model\n",
    "Here is a decision tree model, again using scikit-learn. See [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decision%20tree#sklearn.tree.DecisionTreeClassifier).\n",
    "The most important parameter is `max_depth`, i.e., the maximum depth of the tree.\n",
    "\n",
    "### Task:\n",
    "- Train a decision tree model with the training data and a maximum depth of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad16f5-79b0-4db1-a564-aff815684dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = # add own code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e1c20",
   "metadata": {},
   "source": [
    "## Evaluate results\n",
    "\n",
    "### Tasks:\n",
    "Just as with the kNN model, the task here is to:\n",
    "- Make predictions based on the test data\n",
    "- Compare these with the actual values using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_survival = # add own code\n",
    "prediction_survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75472da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "sb.heatmap(confusion_matrix(# add own code),\n",
    "           annot=True, cmap=\"Blues\", cbar=False, fmt=\".0f\",\n",
    "           xticklabels=[\"Died\", \"Survived\"],\n",
    "           yticklabels=[\"Died\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(# add own code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7929f0",
   "metadata": {},
   "source": [
    "## Interesting facts about decision trees:\n",
    "A popular feature of decision trees is that we can also display the trees themselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f133c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_tree(tree, feature_names=feature_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad2a68",
   "metadata": {},
   "source": [
    "### Task:\n",
    "- Run the same game again, but this time with a tree depth of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba343e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(# add own code)\n",
    "# train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_survival = # add own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "sb.heatmap(confusion_matrix(# add own code),\n",
    "           annot=True, cmap=\"Blues\", cbar=False, fmt=\".0f\",\n",
    "           xticklabels=[\"Died\", \"Survived\"],\n",
    "           yticklabels=[\"Died\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c26535",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6251f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_tree(tree, feature_names=feature_names, filled=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
